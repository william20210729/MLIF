
STEPS밐
1. Install software or packages믿on Windows, Mac , or Linux밀
a) python 3.8.5  
b) pytorch 1.11.0  
c) tqdm 4.50.2 
d) sklearn 0.23.2
e) tensorboardX 2.6

2. Prepare code and data
a) Download code from http://XXXXX. Unzip the files. 
b) Download the base model file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz . Unzip it밃and then put it under the folder bert_pretrain/.
c) Prepare data and put data under the folder zongdiaosolution6/data/. See Section 4 for details, please.
d) If you are about to test using our data (The work order text data contains sensitive information, so it is not published on the network.), please send an email to contact us ###@XXXX.XXXX
모모
3. Run
python run_MLIF_BERT.py

Before running the model, you can update parameters in the file run_MLIF_BERT.py밐
모모### parameters settings
모모parser = argparse.ArgumentParser(description='the MLIF-BERT Model')
모모parser.add_argument('--model', type=str, default="MLIF_BERT", help='choose a model밃models are put in the models folder') 
모모parser.add_argument('--num_epochs', type=str, default="3", help='set epoch') 
모모parser.add_argument('--batch_size', type=str, default="128", help='set batch size') 
모모parser.add_argument('--pad_size', type=str, default="32", help='set pad size') 
모모parser.add_argument('--learning_rate', type=str, default="5e-5", help='set learning rate') 
모모parser.add_argument('--dataset_folder', type=str, default='zongdiaosolution6', help='set the folder where data is믿must have a data sub-folder밀') 
모모parser.add_argument('--layercount', type=str, default="12", help='the number of top CLS')

4. Data format
Users need to prepare four files.

In the class.txt file, one category name per line (demo in Chinese).

In the dev.txt/test.txt/train.txt file, each line is a sample. First is the text content, then the tab separator, and then the category number.





